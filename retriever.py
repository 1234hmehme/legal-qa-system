# retriever.py
# -*- coding: utf-8 -*-
"""
Retriever for Vietnamese Law QA System (Weaviate v4)
- Hybrid retrieval (BM25 + vector) on LawChunks
- Cross-Encoder reranking (BAAI/bge-reranker-v2-m3)
- Embed model: BAAI/bge-m3 (same as indexing)
- Rerank tr√™n: rerank_title + rerank_body
- Context cho LLM: law + chapter + section + ƒêi·ªÅu + Kho·∫£n + ƒêi·ªÉm + n·ªôi dung
"""

import os
import re
import torch
import weaviate
from sentence_transformers import SentenceTransformer, CrossEncoder

# ---------------- ENV SETUP ----------------
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
torch.backends.mps.is_available = lambda: False  # type: ignore
torch.set_num_threads(1)

# ---------------- MODEL LOADING ----------------
print("üîπ Loading embedding model (BAAI/bge-m3)...")
emb_model = SentenceTransformer("BAAI/bge-m3", device="cpu")
print("‚úì Embedding model loaded on CPU")

print("üîπ Loading reranker model (BAAI/bge-reranker-v2-m3)...")
reranker = CrossEncoder("BAAI/bge-reranker-v2-m3", device="cpu")
print("‚úì Reranker model loaded on CPU")

# ---------------- WEAVIATE CONNECTION ----------------
print("üåê Connecting to Weaviate...")
client = weaviate.connect_to_local()
collection = client.collections.get("LawChunks")
print("‚úì Connected to collection: LawChunks")

# ---------------- UTILS ----------------
LEGAL_HINT_RE = re.compile(r"\b(Ch∆∞∆°ng|M·ª•c|ƒêi·ªÅu|Kho·∫£n|ƒêi·ªÉm)\s+[IVXLC\d]+", re.IGNORECASE)
# Ch·ªâ match s·ªë khi ƒëi k√®m v·ªõi t·ª´ kh√≥a ph√°p l√Ω: "ƒêi·ªÅu 15", "Kho·∫£n 2", v.v.

# Ph√°t hi·ªán th√¥ng tin s·ªë c·ª• th·ªÉ trong query (gi·ªù, km/h, tri·ªáu, s·ªë l·∫ßn...)
NUMERIC_INFO_RE = re.compile(r"\b(\d+)\s*(gi·ªù|km/h|tri·ªáu|ngh√¨n|ƒë·ªìng|l·∫ßn|ng√†y|th√°ng|nƒÉm|%|ph·∫ßn trƒÉm|cm3|cc|t·∫•n|km|m|kW)\b", re.IGNORECASE)


def tune_alpha_and_pool(query: str, base_alpha: float = 0.55, k: int = 5):
    """
    Heuristic:
      - Query c√≥ ch·ªâ m·ª•c ph√°p l√Ω c·ª• th·ªÉ (ƒêi·ªÅu X, Kho·∫£n Y...) ‚Üí thi√™n BM25 m·∫°nh
      - Query c√≥ th√¥ng tin s·ªë c·ª• th·ªÉ (22 gi·ªù, 120 km/h...) ‚Üí thi√™n BM25 v·ª´a
      - Query m√¥ t·∫£ h√†nh vi thu·∫ßn ng√¥n ng·ªØ t·ª± nhi√™n ‚Üí thi√™n semantic
    """
    alpha = base_alpha
    initial_k = max(10, k * 5)

    if LEGAL_HINT_RE.search(query):
        # C√≥ ch·ªâ m·ª•c ph√°p l√Ω c·ª• th·ªÉ nh∆∞ "ƒêi·ªÅu 15", "Kho·∫£n 2"
        alpha = max(0.30, base_alpha - 0.25)   # thi√™n BM25 m·∫°nh nh·∫•t
        initial_k = max(15, k * 6)
    elif NUMERIC_INFO_RE.search(query):
        # C√≥ th√¥ng tin s·ªë c·ª• th·ªÉ nh∆∞ "22 gi·ªù", "120 km/h"
        alpha = max(0.40, base_alpha - 0.15)   # thi√™n BM25 v·ª´a ph·∫£i
        initial_k = max(12, k * 5)
    else:
        # Query ng√¥n ng·ªØ t·ª± nhi√™n thu·∫ßn t√∫y
        alpha = min(0.75, base_alpha + 0.20)   # thi√™n semantic m·∫°nh
        initial_k = max(10, k * 4)

    return alpha, initial_k


# ---------------- RETRIEVAL FUNCTION ----------------
def retrieve(question: str, k: int = 5, base_alpha: float = 0.55):
    """
    Hybrid retrieval + Cross-Encoder reranking
    Returns: (context, sources)
    """
    print(f"\nüîç Retrieving for question: {question}")

    # 1) Heuristic alpha & pool size
    alpha, initial_k = tune_alpha_and_pool(question, base_alpha=base_alpha, k=k)
    print(f"   ‚ñ∂ alpha={alpha:.2f}, initial_k={initial_k}")

    # 2) Encode question ‚Üí dense vector
    qv = emb_model.encode([question], normalize_embeddings=True).astype("float32")

    # 3) Hybrid search (Weaviate v4)
    resp = collection.query.hybrid(
        query=question,
        vector=qv[0].tolist(),
        alpha=alpha,
        limit=initial_k,
        return_properties=[
            "law", "law_code",
            "chapter", "section",
            "article_no", "article_title",
            "clause_no", "point", "bullet_idx",
            "granularity",
            "header", "display_citation",
            "path_text",
            "clause_head",
            "text",
            "rerank_title", "rerank_body",
            "source_file",
        ],
    )

    candidates = []
    for obj in resp.objects or []:
        p = obj.properties or {}
        rr_title = (p.get("rerank_title") or "").strip()
        rr_body = (p.get("rerank_body") or "").strip()

        # fallback n·∫øu body r·ªóng
        if not rr_body:
            rr_body = (p.get("text") or "").strip()

        rerank_text = (rr_title + "\n" + rr_body).strip()
        if not rerank_text:
            continue

        candidates.append(
            {
                "rerank_text": rerank_text,
                "props": p,
            }
        )

    if not candidates:
        print("‚ö†Ô∏è No candidates found.")
        return "", []

    # 4) Cross-Encoder rerank
    print(f"üí° Reranking {len(candidates)} candidates...")
    pairs = [[question, c["rerank_text"]] for c in candidates]
    scores = reranker.predict(pairs)
    for i, c in enumerate(candidates):
        c["rerank_score"] = float(scores[i])

    topk = sorted(candidates, key=lambda x: x["rerank_score"], reverse=True)[:k]

    # 5) Compose context + sources cho LLM
    contexts, sources = [], []
    for c in topk:
        p = c["props"]

        law = (p.get("law") or "").strip()
        chapter = (p.get("chapter") or "").strip()
        section = (p.get("section") or "").strip()
        article_no = (p.get("article_no") or "").strip()
        article_title = (p.get("article_title") or "").strip()
        clause_no = (p.get("clause_no") or "")  # TEXT trong schema
        clause_head = (p.get("clause_head") or "").strip()
        point = (p.get("point") or "").strip()
        body_for_ctx = (p.get("text") or "").strip()  # d√πng text g·ªëc cho context

        lines = []

        # Lu·∫≠t / ch∆∞∆°ng / m·ª•c / ƒëi·ªÅu
        if law:
            lines.append(law)
        if chapter:
            lines.append(chapter)
        if section:
            lines.append(section)
        if article_no or article_title:
            art_line = f"ƒêi·ªÅu {article_no}".strip()
            if article_title:
                art_line += f". {article_title}"
            lines.append(art_line)

        # Ph√¢n bi·ªát 2 tr∆∞·ªùng h·ª£p: c√≥ ƒêi·ªÉm hay kh√¥ng
        if point:
            # üëâ LEAF = ƒêI·ªÇM: C·∫¶N c·∫£ n·ªôi dung kho·∫£n cha + n·ªôi dung ƒëi·ªÉm
            if clause_no and clause_head:
                lines.append(f"Kho·∫£n {clause_no}. {clause_head}")
            elif clause_no:
                lines.append(f"Kho·∫£n {clause_no}")

            lines.append(f"ƒêi·ªÉm {point})")

            if body_for_ctx:
                lines.append(body_for_ctx)

        else:
            # üëâ LEAF = KHO·∫¢N (kh√¥ng c√≥ ƒëi·ªÉm): ch·ªâ ghi label + n·ªôi dung kho·∫£n,
            # KH√îNG l·∫∑p l·∫°i clause_head n·∫øu n√≥ g·∫ßn tr√πng text
            if clause_no:
                lines.append(f"Kho·∫£n {clause_no}")
            if body_for_ctx:
                lines.append(body_for_ctx)

        ctx_chunk = "\n".join(lines).strip()
        if ctx_chunk:
            contexts.append(ctx_chunk)

        src = p.get("display_citation") or p.get("header", "") or ""
        sources.append(f"{law} ‚Äì {src}" if law and src else (src or law))

    context = "\n\n".join(contexts)
    print(f"‚úÖ Retrieved {len(contexts)} top chunks")
    return context, sources


